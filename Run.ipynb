{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDEX 5\n",
    "MAX_SEQ_LENGTH_TRANSFORM = 40\n",
    "NUM_FEATURES_TRANSFORM = 1024\n",
    "\n",
    "#INDEX 10 (GRU) y 11 (LSTM)\n",
    "MAX_SEQ_LENGTH_RNN = 40\n",
    "NUM_FEATURES_RNN = 2048\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN (DenseNet121 or InceptionV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "def crop_center(frame):\n",
    "    cropped = center_crop_layer(frame[None, ...])\n",
    "    cropped = cropped.numpy().squeeze()\n",
    "    return cropped\n",
    "\n",
    "def IsBlack(frame, y, x):\n",
    "    height, width, _ = frame.shape\n",
    "    cond = True\n",
    "    r = 2\n",
    "    i = -r\n",
    "    j = -r\n",
    "    while i < r and i + y < height and i + y > 0 and cond:\n",
    "        j = 0\n",
    "        while j < r and j + x < width and j + x > 0 and cond:\n",
    "            if frame[y+i, x+j].any() != 0:\n",
    "                cond = False\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "              \n",
    "    return cond\n",
    "\n",
    "def get_dimension(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    halfWidth = int(width / 2)\n",
    "    halfHeight = int(height / 2)\n",
    "\n",
    "    up = 0\n",
    "    down = height - 1\n",
    "    left = halfWidth - 1\n",
    "    right = halfWidth + 1\n",
    "\n",
    "    while IsBlack(frame, up, halfWidth) and up < halfHeight:\n",
    "            up = up + 1\n",
    "    while IsBlack(frame, down, halfWidth) and down > halfHeight:\n",
    "            down = down - 1\n",
    "\n",
    "    while not IsBlack(frame, up + 5, left) and left > 0:\n",
    "            left = left - 1\n",
    "    while not IsBlack(frame, up + 5, right) and right < width:\n",
    "            right = right + 1\n",
    "\n",
    "    up = up - 1\n",
    "    down = down + 1\n",
    "    left = left - 1\n",
    "    right = right + 1\n",
    "\n",
    "    return right, up, left, down\n",
    "\n",
    "def load_video(path, max_frames=0):\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "\n",
    "    try:\n",
    "        ret, frame = cap.read()\n",
    "        right, up, left, down = get_dimension(frame)\n",
    "        while True:\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = frame[up:down, left:right]\n",
    "            frame = crop_center(frame)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break            \n",
    "            ret, frame = cap.read()\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.DenseNet121(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.densenet.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor_TRANSFORMER = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor_RNN = build_feature_extractor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_29 (InputLayer)       [(None, None, None)]      0         \n",
      "                                                                 \n",
      " frame_position_embedding (P  (None, None, 1024)       40960     \n",
      " ositionalEmbedding)                                             \n",
      "                                                                 \n",
      " transformer_layer (Transfor  (None, None, 1024)       4211716   \n",
      " merEncoder)                                                     \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 1024)             0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 4)                 4100      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,256,776\n",
      "Trainable params: 4,256,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The inputs are of shape: `(batch_size, frames, num_features)`\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return inputs + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        mask = tf.reduce_any(tf.cast(inputs, \"bool\"), axis=-1)\n",
    "        return mask\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=tf.nn.gelu), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "def get_compiled_model():\n",
    "    sequence_length = MAX_SEQ_LENGTH_TRANSFORM\n",
    "    embed_dim = NUM_FEATURES_TRANSFORM\n",
    "    dense_dim = 4\n",
    "    num_heads = 1\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None))\n",
    "    x = PositionalEmbedding(\n",
    "        sequence_length, embed_dim, name=\"frame_position_embedding\"\n",
    "    )(inputs)\n",
    "    x = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"transformer_layer\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = f\"Model/Transformer/video_classifier\"\n",
    "    \n",
    "    model = get_compiled_model()\n",
    "    model.load_weights(filepath)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "TRANSFORMER_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 40, 2048)]   0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    (None, 40, 16)       99168       ['input_27[0][0]',               \n",
      "                                                                  'input_28[0][0]']               \n",
      "                                                                                                  \n",
      " gru_3 (GRU)                    (None, 8)            624         ['gru_2[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 8)            0           ['gru_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8)            72          ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 4)            36          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99,900\n",
      "Trainable params: 99,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    \n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH_RNN, NUM_FEATURES_RNN))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH_RNN,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = f\"Model/GRU/video_classifier\"\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    seq_model.load_weights(filepath)\n",
    "    seq_model.summary()\n",
    "\n",
    "    return seq_model\n",
    "\n",
    "GRU_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 40, 2048)]   0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 40, 16)       132160      ['input_25[0][0]',               \n",
      "                                                                  'input_26[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 8)            800         ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 8)            0           ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 8)            72          ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 4)            36          ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 133,068\n",
      "Trainable params: 133,068\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH_RNN, NUM_FEATURES_RNN))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH_RNN,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.LSTM(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.LSTM(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = f\"Model/LSTM/video_classifier\"\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    seq_model.load_weights(filepath)\n",
    "    seq_model.summary()\n",
    "\n",
    "    return seq_model\n",
    "\n",
    "LSTM_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(index):\n",
    "    if (index == 0):\n",
    "        return feature_extractor_TRANSFORMER, MAX_SEQ_LENGTH_TRANSFORM, NUM_FEATURES_TRANSFORM\n",
    "    else:\n",
    "        return feature_extractor_RNN, MAX_SEQ_LENGTH_RNN, NUM_FEATURES_RNN\n",
    "    \n",
    "def get_model(index):\n",
    "    if (index == 0):\n",
    "        return TRANSFORMER_model\n",
    "    if (index == 1):\n",
    "        return GRU_model\n",
    "    else:\n",
    "        return LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_video(frames, index):\n",
    "    feature_extractor, max_seq_length, num_features = get_type(index)\n",
    "    frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    if len(frames) < max_seq_length:\n",
    "        diff = max_seq_length - len(frames)\n",
    "        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n",
    "        frames = np.concatenate(frames, padding)\n",
    "\n",
    "    frames = frames[None, ...]\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(max_seq_length, video_length)\n",
    "        for j in range(length):\n",
    "            if np.mean(batch[j, :]) > 0.0:\n",
    "                frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :], verbose=0)\n",
    "            else:\n",
    "                frame_features[i, j, :] = 0.0\n",
    "        frame_mask[i, :length] = 1\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def predict_action(path, index):\n",
    "\n",
    "    model = get_model(index)\n",
    "\n",
    "    class_vocab = np.arange(CLASSES)\n",
    "\n",
    "    frames = load_video(path)\n",
    "    \n",
    "    frame_features, frame_mask = prepare_single_video(frames, index)\n",
    "\n",
    "    if (index == 0):\n",
    "        probabilities = model.predict(frame_features)[0]\n",
    "    else:        \n",
    "        probabilities = model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    print(class_vocab)\n",
    "    print(probabilities)\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[0 1 2 3]\n",
      "[0.06462812 0.23497145 0.02162581 0.6787746 ]\n",
      "  3: 67.88%\n",
      "  1: 23.50%\n",
      "  0:  6.46%\n",
      "  2:  2.16%\n"
     ]
    }
   ],
   "source": [
    "test_video = f\"D://Facultad//PPS//Proyecto Ecografia//Noviembre2020//PAC15//PRE.mov\"\n",
    "\n",
    "print(\"TRANSFORMER:\")\n",
    "test_frames = predict_action(test_video, 0)\n",
    "print(\"GRU:\")\n",
    "test_frames = predict_action(test_video, 1)\n",
    "print(\"LSTM:\")\n",
    "test_frames = predict_action(test_video, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
